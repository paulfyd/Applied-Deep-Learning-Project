{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from random import randint\n",
    "import requests\n",
    "import urllib.request\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import splitfolders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "brands = ['chevrolet', 'renault', 'toyota', 'mazda', 'ford', 'kia', 'nissan', 'volkswagen', 'mercedes-benz', 'bmw']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrap the links of the images and store them in a `.csv` for each brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AppURLopener(urllib.request.FancyURLopener):\n",
    "    version = \"Mozilla/5.0\"\n",
    "\n",
    "\n",
    "def get_attr(li, attr):\n",
    "    list_attr = [ele[attr] for ele in li]\n",
    "    return list_attr\n",
    "\n",
    "\n",
    "def get_data(url):\n",
    "    opener = AppURLopener()\n",
    "    try:\n",
    "        html = opener.open(url)\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        return soup, True\n",
    "    except:\n",
    "        randSleep = randint(60, 120)\n",
    "        print(f'Access to the site denied, waiting {randSleep} seconds.')\n",
    "        time.sleep(randSleep)\n",
    "        return None, False\n",
    "\n",
    "\n",
    "def image_link_scraping(brands):\n",
    "    for brand in brands:\n",
    "        list_ads = []\n",
    "        success = False\n",
    "        for idx in range(1, 500, 48): # Each page has 48 ads of cars\n",
    "            while not(success):\n",
    "                soup_page, success = get_data(f'https://carros.tucarro.com.co/{brand}/_Desde_{idx}_NoIndex_True')\n",
    "            link_ads = soup_page.find_all('a', class_='ui-search-result__content ui-search-link')\n",
    "            if link_ads != []:\n",
    "                list_ads += get_attr(link_ads, 'href')\n",
    "            success = False\n",
    "        print(f'list of ads for {brand} created.')\n",
    "        n = len(list_ads)\n",
    "        data_vehic = []\n",
    "        success = False\n",
    "        i = 0\n",
    "        c = 0\n",
    "        while i < n:\n",
    "            # Wait until we have access to the site\n",
    "            while not(success):\n",
    "                soup_annonce, success = get_data(list_ads[i]) \n",
    "            try:\n",
    "                # We store 2 images of the same car\n",
    "                img_1 = soup_annonce.find_all('figure', class_='ui-pdp-gallery__figure')[0].img['data-zoom']\n",
    "                img_2 = soup_annonce.find_all('figure', class_='ui-pdp-gallery__figure')[randint(1,2)].img['data-zoom']\n",
    "                data_vehic.append(img_1)\n",
    "                data_vehic.append(img_2)\n",
    "                # caract = soup_annonce.find_all('span', class_='andes-table__column--value')\n",
    "                # We store the brand of the car\n",
    "                # marque = caract[0].text\n",
    "                # We store the type of the car\n",
    "                # type = caract[-2].text\n",
    "                #data_vehic.append((marque, type, img_1))\n",
    "                #data_vehic.append((marque, type, img_2))\n",
    "                if (c  % 200 == 0):\n",
    "                    print(f'{c} links of ads have been scraped of {brand}.')\n",
    "                c += 2\n",
    "                i += 1\n",
    "            except:\n",
    "                i += 1\n",
    "            success = False\n",
    "        df = pd.DataFrame(data_vehic, columns=['img_url'])\n",
    "        df.to_csv(f'./links-brand/{brand}.csv', sep=';', index=False)\n",
    "        print(f'csv for {brand} has been created.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carlos/anaconda3/envs/data_science/lib/python3.7/site-packages/ipykernel_launcher.py:13: DeprecationWarning: AppURLopener style of invoking requests is deprecated. Use newer urlopen functions/methods\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list of ads for chevrolet created.\n",
      "0 links of ads have been scraped of chevrolet.\n",
      "200 links of ads have been scraped of chevrolet.\n",
      "Access to the site denied, waiting 63 seconds.\n",
      "Access to the site denied, waiting 64 seconds.\n",
      "Access to the site denied, waiting 105 seconds.\n",
      "Access to the site denied, waiting 61 seconds.\n",
      "400 links of ads have been scraped of chevrolet.\n",
      "Access to the site denied, waiting 108 seconds.\n",
      "Access to the site denied, waiting 62 seconds.\n",
      "Access to the site denied, waiting 108 seconds.\n",
      "600 links of ads have been scraped of chevrolet.\n",
      "800 links of ads have been scraped of chevrolet.\n",
      "Access to the site denied, waiting 104 seconds.\n",
      "Access to the site denied, waiting 99 seconds.\n",
      "Access to the site denied, waiting 85 seconds.\n",
      "Access to the site denied, waiting 67 seconds.\n",
      "csv for chevrolet has been created.\n",
      "list of ads for renault created.\n",
      "0 links of ads have been scraped of renault.\n",
      "200 links of ads have been scraped of renault.\n",
      "Access to the site denied, waiting 111 seconds.\n",
      "Access to the site denied, waiting 97 seconds.\n",
      "Access to the site denied, waiting 100 seconds.\n",
      "400 links of ads have been scraped of renault.\n",
      "600 links of ads have been scraped of renault.\n",
      "Access to the site denied, waiting 75 seconds.\n",
      "Access to the site denied, waiting 119 seconds.\n",
      "Access to the site denied, waiting 60 seconds.\n",
      "Access to the site denied, waiting 89 seconds.\n",
      "800 links of ads have been scraped of renault.\n",
      "1000 links of ads have been scraped of renault.\n",
      "csv for renault has been created.\n",
      "Access to the site denied, waiting 101 seconds.\n",
      "Access to the site denied, waiting 99 seconds.\n",
      "Access to the site denied, waiting 73 seconds.\n",
      "list of ads for toyota created.\n",
      "0 links of ads have been scraped of toyota.\n",
      "200 links of ads have been scraped of toyota.\n",
      "Access to the site denied, waiting 84 seconds.\n",
      "Access to the site denied, waiting 91 seconds.\n",
      "Access to the site denied, waiting 109 seconds.\n",
      "400 links of ads have been scraped of toyota.\n",
      "600 links of ads have been scraped of toyota.\n",
      "Access to the site denied, waiting 111 seconds.\n",
      "Access to the site denied, waiting 66 seconds.\n",
      "Access to the site denied, waiting 100 seconds.\n",
      "Access to the site denied, waiting 60 seconds.\n",
      "800 links of ads have been scraped of toyota.\n",
      "csv for toyota has been created.\n",
      "list of ads for mazda created.\n",
      "0 links of ads have been scraped of mazda.\n",
      "Access to the site denied, waiting 109 seconds.\n",
      "Access to the site denied, waiting 75 seconds.\n",
      "Access to the site denied, waiting 76 seconds.\n",
      "Access to the site denied, waiting 62 seconds.\n",
      "200 links of ads have been scraped of mazda.\n",
      "Access to the site denied, waiting 104 seconds.\n",
      "Access to the site denied, waiting 106 seconds.\n",
      "Access to the site denied, waiting 81 seconds.\n",
      "Access to the site denied, waiting 97 seconds.\n",
      "400 links of ads have been scraped of mazda.\n",
      "600 links of ads have been scraped of mazda.\n",
      "Access to the site denied, waiting 84 seconds.\n",
      "Access to the site denied, waiting 65 seconds.\n",
      "Access to the site denied, waiting 105 seconds.\n",
      "Access to the site denied, waiting 68 seconds.\n",
      "800 links of ads have been scraped of mazda.\n",
      "csv for mazda has been created.\n",
      "list of ads for ford created.\n",
      "0 links of ads have been scraped of ford.\n",
      "Access to the site denied, waiting 98 seconds.\n",
      "Access to the site denied, waiting 118 seconds.\n",
      "Access to the site denied, waiting 93 seconds.\n",
      "200 links of ads have been scraped of ford.\n",
      "400 links of ads have been scraped of ford.\n",
      "Access to the site denied, waiting 101 seconds.\n",
      "Access to the site denied, waiting 63 seconds.\n",
      "Access to the site denied, waiting 101 seconds.\n",
      "600 links of ads have been scraped of ford.\n",
      "Access to the site denied, waiting 68 seconds.\n",
      "Access to the site denied, waiting 62 seconds.\n",
      "Access to the site denied, waiting 115 seconds.\n",
      "Access to the site denied, waiting 94 seconds.\n",
      "800 links of ads have been scraped of ford.\n",
      "csv for ford has been created.\n",
      "list of ads for kia created.\n",
      "0 links of ads have been scraped of kia.\n",
      "Access to the site denied, waiting 86 seconds.\n",
      "Access to the site denied, waiting 99 seconds.\n",
      "Access to the site denied, waiting 103 seconds.\n",
      "200 links of ads have been scraped of kia.\n",
      "Access to the site denied, waiting 60 seconds.\n",
      "Access to the site denied, waiting 75 seconds.\n",
      "Access to the site denied, waiting 60 seconds.\n",
      "Access to the site denied, waiting 76 seconds.\n",
      "400 links of ads have been scraped of kia.\n",
      "600 links of ads have been scraped of kia.\n",
      "Access to the site denied, waiting 105 seconds.\n",
      "Access to the site denied, waiting 60 seconds.\n",
      "Access to the site denied, waiting 112 seconds.\n",
      "800 links of ads have been scraped of kia.\n",
      "csv for kia has been created.\n",
      "list of ads for nissan created.\n",
      "0 links of ads have been scraped of nissan.\n",
      "Access to the site denied, waiting 79 seconds.\n",
      "Access to the site denied, waiting 120 seconds.\n",
      "Access to the site denied, waiting 103 seconds.\n",
      "200 links of ads have been scraped of nissan.\n",
      "Access to the site denied, waiting 110 seconds.\n",
      "Access to the site denied, waiting 69 seconds.\n",
      "Access to the site denied, waiting 103 seconds.\n",
      "400 links of ads have been scraped of nissan.\n",
      "600 links of ads have been scraped of nissan.\n",
      "Access to the site denied, waiting 112 seconds.\n",
      "Access to the site denied, waiting 104 seconds.\n",
      "Access to the site denied, waiting 70 seconds.\n",
      "800 links of ads have been scraped of nissan.\n",
      "Access to the site denied, waiting 103 seconds.\n",
      "Access to the site denied, waiting 98 seconds.\n",
      "Access to the site denied, waiting 81 seconds.\n",
      "csv for nissan has been created.\n",
      "list of ads for volkswagen created.\n",
      "0 links of ads have been scraped of volkswagen.\n",
      "Access to the site denied, waiting 118 seconds.\n",
      "200 links of ads have been scraped of volkswagen.\n",
      "400 links of ads have been scraped of volkswagen.\n",
      "Access to the site denied, waiting 113 seconds.\n",
      "Access to the site denied, waiting 107 seconds.\n",
      "600 links of ads have been scraped of volkswagen.\n",
      "Access to the site denied, waiting 61 seconds.\n",
      "Access to the site denied, waiting 77 seconds.\n",
      "Access to the site denied, waiting 109 seconds.\n",
      "800 links of ads have been scraped of volkswagen.\n",
      "csv for volkswagen has been created.\n",
      "list of ads for mercedes-benz created.\n",
      "0 links of ads have been scraped of mercedes-benz.\n",
      "Access to the site denied, waiting 84 seconds.\n",
      "Access to the site denied, waiting 75 seconds.\n",
      "Access to the site denied, waiting 75 seconds.\n",
      "200 links of ads have been scraped of mercedes-benz.\n",
      "Access to the site denied, waiting 65 seconds.\n",
      "Access to the site denied, waiting 100 seconds.\n",
      "Access to the site denied, waiting 117 seconds.\n",
      "400 links of ads have been scraped of mercedes-benz.\n",
      "600 links of ads have been scraped of mercedes-benz.\n",
      "Access to the site denied, waiting 106 seconds.\n",
      "Access to the site denied, waiting 83 seconds.\n",
      "Access to the site denied, waiting 109 seconds.\n",
      "800 links of ads have been scraped of mercedes-benz.\n",
      "csv for mercedes-benz has been created.\n",
      "Access to the site denied, waiting 96 seconds.\n",
      "Access to the site denied, waiting 86 seconds.\n",
      "Access to the site denied, waiting 102 seconds.\n",
      "list of ads for bmw created.\n",
      "0 links of ads have been scraped of bmw.\n",
      "200 links of ads have been scraped of bmw.\n",
      "Access to the site denied, waiting 77 seconds.\n",
      "Access to the site denied, waiting 87 seconds.\n",
      "Access to the site denied, waiting 102 seconds.\n",
      "Access to the site denied, waiting 102 seconds.\n",
      "400 links of ads have been scraped of bmw.\n",
      "600 links of ads have been scraped of bmw.\n",
      "Access to the site denied, waiting 60 seconds.\n",
      "Access to the site denied, waiting 111 seconds.\n",
      "Access to the site denied, waiting 72 seconds.\n",
      "Access to the site denied, waiting 66 seconds.\n",
      "800 links of ads have been scraped of bmw.\n",
      "csv for bmw has been created.\n"
     ]
    }
   ],
   "source": [
    "image_link_scraping(brands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the images from the links in each `.csv` and store the images in a directory with the brand name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_imgs(brands):\n",
    "    try:\n",
    "        os.mkdir('./imgs-brand')\n",
    "    except:\n",
    "        pass\n",
    "    for brand in brands:\n",
    "        df = pd.read_csv(f'./links-brand/{brand}.csv', sep=';')\n",
    "        urls = df['img_url']\n",
    "        try:\n",
    "            os.mkdir(f'./imgs-brand/{brand}')\n",
    "        except:\n",
    "            pass\n",
    "        counter = 0\n",
    "        for url in urls:\n",
    "            try:\n",
    "                file_name = f'./imgs-brand/{brand}/{brand}' + str(counter) + '.jpg'\n",
    "                response = requests.get(url)\n",
    "                file = open(file_name, 'wb')\n",
    "                file.write(response.content)\n",
    "                file.close()\n",
    "                counter += 1\n",
    "            except:\n",
    "                pass\n",
    "        print(f'{counter} images have successfully been downloaded for {brand}.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "962 images have successfully been downloaded for chevrolet.\n",
      "1004 images have successfully been downloaded for renault.\n",
      "956 images have successfully been downloaded for toyota.\n",
      "988 images have successfully been downloaded for mazda.\n",
      "1000 images have successfully been downloaded for ford.\n",
      "972 images have successfully been downloaded for kia.\n",
      "952 images have successfully been downloaded for nissan.\n",
      "978 images have successfully been downloaded for volkswagen.\n",
      "974 images have successfully been downloaded for mercedes-benz.\n",
      "978 images have successfully been downloaded for bmw.\n"
     ]
    }
   ],
   "source": [
    "download_imgs(brands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the test / train / val folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_train_val():\n",
    "    try:\n",
    "        os.mkdir('./data')\n",
    "    except:\n",
    "        pass\n",
    "    splitfolders.ratio(\n",
    "        './imgs-brand/',\n",
    "        output='./data',\n",
    "        ratio=(.8, .1, .1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 9764 files [25:06,  6.48 files/s]\n"
     ]
    }
   ],
   "source": [
    "test_train_val()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "df5110a174fb920e8a1a164036d27db828b7b074fe5c7312a2fadadaa5d71696"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
